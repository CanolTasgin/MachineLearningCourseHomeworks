{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS412_HW5_ccanol.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPnUZjn83mPM"
      },
      "source": [
        "# CS412 Homework 5\n",
        "Celal Canol Taşgın / 20761 / ccanol@sabanciuniv.edu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_zv7aEO3cIY"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Flatten the images\n",
        "\n",
        "image_vector_size = 28*28\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], image_vector_size)\n",
        "\n",
        "x_test = x_test.reshape(x_test.shape[0], image_vector_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX_8dxKhfyF8"
      },
      "source": [
        "# run1 & Used tanh as activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPYMUGB5fnUj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "988c4d47-a799-4e08-8b59-ac6438e24a43"
      },
      "source": [
        "results = []\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y = make_classification(n_samples=1000, random_state=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
        "clf = MLPClassifier(hidden_layer_sizes = (784,300,100,1),alpha=0.0001,solver='sgd', verbose=10,activation='tanh',max_iter=300,random_state=1).fit(X_train, y_train)\n",
        "clf.predict_proba(X_test)\n",
        "clf.predict(X_test)\n",
        "results.append(['run1/tanh TEST',clf.score(X_test, y_test)])\n",
        "clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.83862204\n",
            "Iteration 2, loss = 0.83648215\n",
            "Iteration 3, loss = 0.83325942\n",
            "Iteration 4, loss = 0.82931376\n",
            "Iteration 5, loss = 0.82473195\n",
            "Iteration 6, loss = 0.82000910\n",
            "Iteration 7, loss = 0.81513572\n",
            "Iteration 8, loss = 0.81004080\n",
            "Iteration 9, loss = 0.80466245\n",
            "Iteration 10, loss = 0.79940938\n",
            "Iteration 11, loss = 0.79424204\n",
            "Iteration 12, loss = 0.78894680\n",
            "Iteration 13, loss = 0.78349344\n",
            "Iteration 14, loss = 0.77834183\n",
            "Iteration 15, loss = 0.77285257\n",
            "Iteration 16, loss = 0.76764695\n",
            "Iteration 17, loss = 0.76239586\n",
            "Iteration 18, loss = 0.75711289\n",
            "Iteration 19, loss = 0.75173925\n",
            "Iteration 20, loss = 0.74651521\n",
            "Iteration 21, loss = 0.74129835\n",
            "Iteration 22, loss = 0.73606560\n",
            "Iteration 23, loss = 0.73080955\n",
            "Iteration 24, loss = 0.72569512\n",
            "Iteration 25, loss = 0.72062890\n",
            "Iteration 26, loss = 0.71547868\n",
            "Iteration 27, loss = 0.71045582\n",
            "Iteration 28, loss = 0.70534920\n",
            "Iteration 29, loss = 0.70030200\n",
            "Iteration 30, loss = 0.69535254\n",
            "Iteration 31, loss = 0.69043812\n",
            "Iteration 32, loss = 0.68558396\n",
            "Iteration 33, loss = 0.68074594\n",
            "Iteration 34, loss = 0.67591728\n",
            "Iteration 35, loss = 0.67132792\n",
            "Iteration 36, loss = 0.66652759\n",
            "Iteration 37, loss = 0.66183413\n",
            "Iteration 38, loss = 0.65729133\n",
            "Iteration 39, loss = 0.65274473\n",
            "Iteration 40, loss = 0.64817754\n",
            "Iteration 41, loss = 0.64376800\n",
            "Iteration 42, loss = 0.63938694\n",
            "Iteration 43, loss = 0.63503298\n",
            "Iteration 44, loss = 0.63073669\n",
            "Iteration 45, loss = 0.62653341\n",
            "Iteration 46, loss = 0.62221665\n",
            "Iteration 47, loss = 0.61815231\n",
            "Iteration 48, loss = 0.61416825\n",
            "Iteration 49, loss = 0.61000589\n",
            "Iteration 50, loss = 0.60602868\n",
            "Iteration 51, loss = 0.60217784\n",
            "Iteration 52, loss = 0.59827842\n",
            "Iteration 53, loss = 0.59451730\n",
            "Iteration 54, loss = 0.59075740\n",
            "Iteration 55, loss = 0.58704811\n",
            "Iteration 56, loss = 0.58344227\n",
            "Iteration 57, loss = 0.57981661\n",
            "Iteration 58, loss = 0.57628344\n",
            "Iteration 59, loss = 0.57287384\n",
            "Iteration 60, loss = 0.56929524\n",
            "Iteration 61, loss = 0.56609509\n",
            "Iteration 62, loss = 0.56273169\n",
            "Iteration 63, loss = 0.55955272\n",
            "Iteration 64, loss = 0.55628461\n",
            "Iteration 65, loss = 0.55307788\n",
            "Iteration 66, loss = 0.55006596\n",
            "Iteration 67, loss = 0.54696579\n",
            "Iteration 68, loss = 0.54397213\n",
            "Iteration 69, loss = 0.54104692\n",
            "Iteration 70, loss = 0.53819380\n",
            "Iteration 71, loss = 0.53527050\n",
            "Iteration 72, loss = 0.53250408\n",
            "Iteration 73, loss = 0.52979680\n",
            "Iteration 74, loss = 0.52702763\n",
            "Iteration 75, loss = 0.52435605\n",
            "Iteration 76, loss = 0.52178253\n",
            "Iteration 77, loss = 0.51925021\n",
            "Iteration 78, loss = 0.51669177\n",
            "Iteration 79, loss = 0.51419122\n",
            "Iteration 80, loss = 0.51178040\n",
            "Iteration 81, loss = 0.50934701\n",
            "Iteration 82, loss = 0.50702890\n",
            "Iteration 83, loss = 0.50467510\n",
            "Iteration 84, loss = 0.50241795\n",
            "Iteration 85, loss = 0.50015876\n",
            "Iteration 86, loss = 0.49794697\n",
            "Iteration 87, loss = 0.49584190\n",
            "Iteration 88, loss = 0.49366133\n",
            "Iteration 89, loss = 0.49153500\n",
            "Iteration 90, loss = 0.48953570\n",
            "Iteration 91, loss = 0.48747900\n",
            "Iteration 92, loss = 0.48555024\n",
            "Iteration 93, loss = 0.48358739\n",
            "Iteration 94, loss = 0.48166539\n",
            "Iteration 95, loss = 0.47978578\n",
            "Iteration 96, loss = 0.47791008\n",
            "Iteration 97, loss = 0.47611587\n",
            "Iteration 98, loss = 0.47432044\n",
            "Iteration 99, loss = 0.47256304\n",
            "Iteration 100, loss = 0.47081276\n",
            "Iteration 101, loss = 0.46916393\n",
            "Iteration 102, loss = 0.46747635\n",
            "Iteration 103, loss = 0.46580437\n",
            "Iteration 104, loss = 0.46414958\n",
            "Iteration 105, loss = 0.46262761\n",
            "Iteration 106, loss = 0.46098810\n",
            "Iteration 107, loss = 0.45944520\n",
            "Iteration 108, loss = 0.45795774\n",
            "Iteration 109, loss = 0.45642565\n",
            "Iteration 110, loss = 0.45495390\n",
            "Iteration 111, loss = 0.45355404\n",
            "Iteration 112, loss = 0.45211883\n",
            "Iteration 113, loss = 0.45072291\n",
            "Iteration 114, loss = 0.44932964\n",
            "Iteration 115, loss = 0.44803904\n",
            "Iteration 116, loss = 0.44669228\n",
            "Iteration 117, loss = 0.44538773\n",
            "Iteration 118, loss = 0.44409212\n",
            "Iteration 119, loss = 0.44280950\n",
            "Iteration 120, loss = 0.44160998\n",
            "Iteration 121, loss = 0.44037269\n",
            "Iteration 122, loss = 0.43918729\n",
            "Iteration 123, loss = 0.43794789\n",
            "Iteration 124, loss = 0.43681288\n",
            "Iteration 125, loss = 0.43562643\n",
            "Iteration 126, loss = 0.43452596\n",
            "Iteration 127, loss = 0.43336663\n",
            "Iteration 128, loss = 0.43230800\n",
            "Iteration 129, loss = 0.43120749\n",
            "Iteration 130, loss = 0.43017106\n",
            "Iteration 131, loss = 0.42912321\n",
            "Iteration 132, loss = 0.42807248\n",
            "Iteration 133, loss = 0.42706369\n",
            "Iteration 134, loss = 0.42605974\n",
            "Iteration 135, loss = 0.42505838\n",
            "Iteration 136, loss = 0.42406333\n",
            "Iteration 137, loss = 0.42311149\n",
            "Iteration 138, loss = 0.42217646\n",
            "Iteration 139, loss = 0.42123422\n",
            "Iteration 140, loss = 0.42032447\n",
            "Iteration 141, loss = 0.41939387\n",
            "Iteration 142, loss = 0.41849818\n",
            "Iteration 143, loss = 0.41763964\n",
            "Iteration 144, loss = 0.41674325\n",
            "Iteration 145, loss = 0.41589018\n",
            "Iteration 146, loss = 0.41506316\n",
            "Iteration 147, loss = 0.41422688\n",
            "Iteration 148, loss = 0.41339949\n",
            "Iteration 149, loss = 0.41264603\n",
            "Iteration 150, loss = 0.41180141\n",
            "Iteration 151, loss = 0.41103165\n",
            "Iteration 152, loss = 0.41024443\n",
            "Iteration 153, loss = 0.40954328\n",
            "Iteration 154, loss = 0.40875394\n",
            "Iteration 155, loss = 0.40809201\n",
            "Iteration 156, loss = 0.40732970\n",
            "Iteration 157, loss = 0.40658653\n",
            "Iteration 158, loss = 0.40588689\n",
            "Iteration 159, loss = 0.40520649\n",
            "Iteration 160, loss = 0.40453585\n",
            "Iteration 161, loss = 0.40380296\n",
            "Iteration 162, loss = 0.40313796\n",
            "Iteration 163, loss = 0.40249606\n",
            "Iteration 164, loss = 0.40183887\n",
            "Iteration 165, loss = 0.40120101\n",
            "Iteration 166, loss = 0.40052807\n",
            "Iteration 167, loss = 0.39993619\n",
            "Iteration 168, loss = 0.39931436\n",
            "Iteration 169, loss = 0.39869462\n",
            "Iteration 170, loss = 0.39806427\n",
            "Iteration 171, loss = 0.39754817\n",
            "Iteration 172, loss = 0.39695597\n",
            "Iteration 173, loss = 0.39633670\n",
            "Iteration 174, loss = 0.39582702\n",
            "Iteration 175, loss = 0.39520478\n",
            "Iteration 176, loss = 0.39470599\n",
            "Iteration 177, loss = 0.39417006\n",
            "Iteration 178, loss = 0.39358694\n",
            "Iteration 179, loss = 0.39309212\n",
            "Iteration 180, loss = 0.39255235\n",
            "Iteration 181, loss = 0.39198671\n",
            "Iteration 182, loss = 0.39151137\n",
            "Iteration 183, loss = 0.39100988\n",
            "Iteration 184, loss = 0.39051057\n",
            "Iteration 185, loss = 0.39002503\n",
            "Iteration 186, loss = 0.38955658\n",
            "Iteration 187, loss = 0.38905548\n",
            "Iteration 188, loss = 0.38862432\n",
            "Iteration 189, loss = 0.38813258\n",
            "Iteration 190, loss = 0.38765680\n",
            "Iteration 191, loss = 0.38721641\n",
            "Iteration 192, loss = 0.38680072\n",
            "Iteration 193, loss = 0.38634399\n",
            "Iteration 194, loss = 0.38590927\n",
            "Iteration 195, loss = 0.38546821\n",
            "Iteration 196, loss = 0.38502539\n",
            "Iteration 197, loss = 0.38464709\n",
            "Iteration 198, loss = 0.38422973\n",
            "Iteration 199, loss = 0.38383865\n",
            "Iteration 200, loss = 0.38341029\n",
            "Iteration 201, loss = 0.38302306\n",
            "Iteration 202, loss = 0.38263748\n",
            "Iteration 203, loss = 0.38222787\n",
            "Iteration 204, loss = 0.38187096\n",
            "Iteration 205, loss = 0.38144592\n",
            "Iteration 206, loss = 0.38108480\n",
            "Iteration 207, loss = 0.38070088\n",
            "Iteration 208, loss = 0.38034063\n",
            "Iteration 209, loss = 0.37998717\n",
            "Iteration 210, loss = 0.37960047\n",
            "Iteration 211, loss = 0.37928976\n",
            "Iteration 212, loss = 0.37889136\n",
            "Iteration 213, loss = 0.37857570\n",
            "Iteration 214, loss = 0.37818327\n",
            "Iteration 215, loss = 0.37787545\n",
            "Iteration 216, loss = 0.37750340\n",
            "Iteration 217, loss = 0.37719533\n",
            "Iteration 218, loss = 0.37688069\n",
            "Iteration 219, loss = 0.37653325\n",
            "Iteration 220, loss = 0.37619859\n",
            "Iteration 221, loss = 0.37584483\n",
            "Iteration 222, loss = 0.37557185\n",
            "Iteration 223, loss = 0.37530010\n",
            "Iteration 224, loss = 0.37494328\n",
            "Iteration 225, loss = 0.37465483\n",
            "Iteration 226, loss = 0.37432384\n",
            "Iteration 227, loss = 0.37403603\n",
            "Iteration 228, loss = 0.37375008\n",
            "Iteration 229, loss = 0.37346252\n",
            "Iteration 230, loss = 0.37316343\n",
            "Iteration 231, loss = 0.37287639\n",
            "Iteration 232, loss = 0.37264378\n",
            "Iteration 233, loss = 0.37229908\n",
            "Iteration 234, loss = 0.37206783\n",
            "Iteration 235, loss = 0.37173335\n",
            "Iteration 236, loss = 0.37149602\n",
            "Iteration 237, loss = 0.37121192\n",
            "Iteration 238, loss = 0.37095671\n",
            "Iteration 239, loss = 0.37068859\n",
            "Iteration 240, loss = 0.37042655\n",
            "Iteration 241, loss = 0.37018970\n",
            "Iteration 242, loss = 0.36989537\n",
            "Iteration 243, loss = 0.36963834\n",
            "Iteration 244, loss = 0.36937871\n",
            "Iteration 245, loss = 0.36915777\n",
            "Iteration 246, loss = 0.36888862\n",
            "Iteration 247, loss = 0.36866279\n",
            "Iteration 248, loss = 0.36842370\n",
            "Iteration 249, loss = 0.36815446\n",
            "Iteration 250, loss = 0.36803110\n",
            "Iteration 251, loss = 0.36772739\n",
            "Iteration 252, loss = 0.36743867\n",
            "Iteration 253, loss = 0.36726311\n",
            "Iteration 254, loss = 0.36698170\n",
            "Iteration 255, loss = 0.36674963\n",
            "Iteration 256, loss = 0.36655437\n",
            "Iteration 257, loss = 0.36634093\n",
            "Iteration 258, loss = 0.36609172\n",
            "Iteration 259, loss = 0.36589528\n",
            "Iteration 260, loss = 0.36564918\n",
            "Iteration 261, loss = 0.36545313\n",
            "Iteration 262, loss = 0.36527851\n",
            "Iteration 263, loss = 0.36499413\n",
            "Iteration 264, loss = 0.36482092\n",
            "Iteration 265, loss = 0.36459846\n",
            "Iteration 266, loss = 0.36438511\n",
            "Iteration 267, loss = 0.36418641\n",
            "Iteration 268, loss = 0.36400425\n",
            "Iteration 269, loss = 0.36375736\n",
            "Iteration 270, loss = 0.36357279\n",
            "Iteration 271, loss = 0.36342007\n",
            "Iteration 272, loss = 0.36317501\n",
            "Iteration 273, loss = 0.36300701\n",
            "Iteration 274, loss = 0.36278217\n",
            "Iteration 275, loss = 0.36257070\n",
            "Iteration 276, loss = 0.36243219\n",
            "Iteration 277, loss = 0.36223568\n",
            "Iteration 278, loss = 0.36200818\n",
            "Iteration 279, loss = 0.36186911\n",
            "Iteration 280, loss = 0.36163425\n",
            "Iteration 281, loss = 0.36145048\n",
            "Iteration 282, loss = 0.36127211\n",
            "Iteration 283, loss = 0.36106929\n",
            "Iteration 284, loss = 0.36090800\n",
            "Iteration 285, loss = 0.36074062\n",
            "Iteration 286, loss = 0.36057617\n",
            "Iteration 287, loss = 0.36035523\n",
            "Iteration 288, loss = 0.36019295\n",
            "Iteration 289, loss = 0.36000750\n",
            "Iteration 290, loss = 0.35984195\n",
            "Iteration 291, loss = 0.35969926\n",
            "Iteration 292, loss = 0.35954959\n",
            "Iteration 293, loss = 0.35930500\n",
            "Iteration 294, loss = 0.35918487\n",
            "Iteration 295, loss = 0.35899793\n",
            "Iteration 296, loss = 0.35888904\n",
            "Iteration 297, loss = 0.35867695\n",
            "Iteration 298, loss = 0.35847443\n",
            "Iteration 299, loss = 0.35834993\n",
            "Iteration 300, loss = 0.35816196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.808"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUTYoaKrfpuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47c83441-9ce1-40cd-d0a8-962a55457411"
      },
      "source": [
        "results.append(['run1/tanh TRAIN',clf.score(X_train, y_train)])\n",
        "clf.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8773333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8yJAohkgGMt"
      },
      "source": [
        "# run1 & Used relu as activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlJYqIOU8cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f86911e3-f714-42d4-e68a-ce4377843112"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y = make_classification(n_samples=1000, random_state=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
        "clf = MLPClassifier(hidden_layer_sizes = (784,300,100,1),alpha=0.0001,solver='sgd', verbose=10,activation='relu',max_iter=300,random_state=1).fit(X_train, y_train)\n",
        "clf.predict_proba(X_test)\n",
        "clf.predict(X_test)\n",
        "results.append(['run1/relu TEST',clf.score(X_test, y_test)])\n",
        "clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.84920657\n",
            "Iteration 2, loss = 0.84814990\n",
            "Iteration 3, loss = 0.84655649\n",
            "Iteration 4, loss = 0.84465474\n",
            "Iteration 5, loss = 0.84241692\n",
            "Iteration 6, loss = 0.84016375\n",
            "Iteration 7, loss = 0.83787895\n",
            "Iteration 8, loss = 0.83549181\n",
            "Iteration 9, loss = 0.83299970\n",
            "Iteration 10, loss = 0.83057919\n",
            "Iteration 11, loss = 0.82830837\n",
            "Iteration 12, loss = 0.82594738\n",
            "Iteration 13, loss = 0.82352783\n",
            "Iteration 14, loss = 0.82137415\n",
            "Iteration 15, loss = 0.81897807\n",
            "Iteration 16, loss = 0.81683380\n",
            "Iteration 17, loss = 0.81467540\n",
            "Iteration 18, loss = 0.81253524\n",
            "Iteration 19, loss = 0.81031923\n",
            "Iteration 20, loss = 0.80826451\n",
            "Iteration 21, loss = 0.80622352\n",
            "Iteration 22, loss = 0.80416600\n",
            "Iteration 23, loss = 0.80214660\n",
            "Iteration 24, loss = 0.80021291\n",
            "Iteration 25, loss = 0.79831382\n",
            "Iteration 26, loss = 0.79637753\n",
            "Iteration 27, loss = 0.79453718\n",
            "Iteration 28, loss = 0.79265331\n",
            "Iteration 29, loss = 0.79083123\n",
            "Iteration 30, loss = 0.78903426\n",
            "Iteration 31, loss = 0.78726661\n",
            "Iteration 32, loss = 0.78556504\n",
            "Iteration 33, loss = 0.78385253\n",
            "Iteration 34, loss = 0.78219648\n",
            "Iteration 35, loss = 0.78062752\n",
            "Iteration 36, loss = 0.77899130\n",
            "Iteration 37, loss = 0.77734996\n",
            "Iteration 38, loss = 0.77583974\n",
            "Iteration 39, loss = 0.77430071\n",
            "Iteration 40, loss = 0.77278515\n",
            "Iteration 41, loss = 0.77129644\n",
            "Iteration 42, loss = 0.76985075\n",
            "Iteration 43, loss = 0.76844776\n",
            "Iteration 44, loss = 0.76702596\n",
            "Iteration 45, loss = 0.76565358\n",
            "Iteration 46, loss = 0.76423053\n",
            "Iteration 47, loss = 0.76293067\n",
            "Iteration 48, loss = 0.76164104\n",
            "Iteration 49, loss = 0.76027779\n",
            "Iteration 50, loss = 0.75902151\n",
            "Iteration 51, loss = 0.75778285\n",
            "Iteration 52, loss = 0.75655695\n",
            "Iteration 53, loss = 0.75534864\n",
            "Iteration 54, loss = 0.75416101\n",
            "Iteration 55, loss = 0.75298100\n",
            "Iteration 56, loss = 0.75185349\n",
            "Iteration 57, loss = 0.75069642\n",
            "Iteration 58, loss = 0.74958580\n",
            "Iteration 59, loss = 0.74854451\n",
            "Iteration 60, loss = 0.74741135\n",
            "Iteration 61, loss = 0.74646679\n",
            "Iteration 62, loss = 0.74539361\n",
            "Iteration 63, loss = 0.74444600\n",
            "Iteration 64, loss = 0.74342417\n",
            "Iteration 65, loss = 0.74239796\n",
            "Iteration 66, loss = 0.74152124\n",
            "Iteration 67, loss = 0.74055051\n",
            "Iteration 68, loss = 0.73962205\n",
            "Iteration 69, loss = 0.73873686\n",
            "Iteration 70, loss = 0.73789655\n",
            "Iteration 71, loss = 0.73698546\n",
            "Iteration 72, loss = 0.73617867\n",
            "Iteration 73, loss = 0.73531817\n",
            "Iteration 74, loss = 0.73447364\n",
            "Iteration 75, loss = 0.73367355\n",
            "Iteration 76, loss = 0.73292315\n",
            "Iteration 77, loss = 0.73217865\n",
            "Iteration 78, loss = 0.73137998\n",
            "Iteration 79, loss = 0.73062401\n",
            "Iteration 80, loss = 0.72992948\n",
            "Iteration 81, loss = 0.72917531\n",
            "Iteration 82, loss = 0.72850681\n",
            "Iteration 83, loss = 0.72781229\n",
            "Iteration 84, loss = 0.72714710\n",
            "Iteration 85, loss = 0.72649922\n",
            "Iteration 86, loss = 0.72579736\n",
            "Iteration 87, loss = 0.72520489\n",
            "Iteration 88, loss = 0.72455885\n",
            "Iteration 89, loss = 0.72391337\n",
            "Iteration 90, loss = 0.72332855\n",
            "Iteration 91, loss = 0.72276912\n",
            "Iteration 92, loss = 0.72215822\n",
            "Iteration 93, loss = 0.72158676\n",
            "Iteration 94, loss = 0.72101207\n",
            "Iteration 95, loss = 0.72048249\n",
            "Iteration 96, loss = 0.71994584\n",
            "Iteration 97, loss = 0.71945241\n",
            "Iteration 98, loss = 0.71890915\n",
            "Iteration 99, loss = 0.71839879\n",
            "Iteration 100, loss = 0.71790800\n",
            "Iteration 101, loss = 0.71742728\n",
            "Iteration 102, loss = 0.71694102\n",
            "Iteration 103, loss = 0.71647573\n",
            "Iteration 104, loss = 0.71600786\n",
            "Iteration 105, loss = 0.71558657\n",
            "Iteration 106, loss = 0.71512889\n",
            "Iteration 107, loss = 0.71466773\n",
            "Iteration 108, loss = 0.71426132\n",
            "Iteration 109, loss = 0.71381768\n",
            "Iteration 110, loss = 0.71341920\n",
            "Iteration 111, loss = 0.71304261\n",
            "Iteration 112, loss = 0.71266394\n",
            "Iteration 113, loss = 0.71227146\n",
            "Iteration 114, loss = 0.71187451\n",
            "Iteration 115, loss = 0.71154400\n",
            "Iteration 116, loss = 0.71116744\n",
            "Iteration 117, loss = 0.71083170\n",
            "Iteration 118, loss = 0.71050144\n",
            "Iteration 119, loss = 0.71014505\n",
            "Iteration 120, loss = 0.70984040\n",
            "Iteration 121, loss = 0.70949686\n",
            "Iteration 122, loss = 0.70919156\n",
            "Iteration 123, loss = 0.70886813\n",
            "Iteration 124, loss = 0.70857140\n",
            "Iteration 125, loss = 0.70828599\n",
            "Iteration 126, loss = 0.70796495\n",
            "Iteration 127, loss = 0.70766332\n",
            "Iteration 128, loss = 0.70741038\n",
            "Iteration 129, loss = 0.70711070\n",
            "Iteration 130, loss = 0.70684252\n",
            "Iteration 131, loss = 0.70659780\n",
            "Iteration 132, loss = 0.70633300\n",
            "Iteration 133, loss = 0.70604960\n",
            "Iteration 134, loss = 0.70581350\n",
            "Iteration 135, loss = 0.70557165\n",
            "Iteration 136, loss = 0.70531732\n",
            "Iteration 137, loss = 0.70508108\n",
            "Iteration 138, loss = 0.70485302\n",
            "Iteration 139, loss = 0.70462978\n",
            "Iteration 140, loss = 0.70439903\n",
            "Iteration 141, loss = 0.70416433\n",
            "Iteration 142, loss = 0.70394948\n",
            "Iteration 143, loss = 0.70372246\n",
            "Iteration 144, loss = 0.70354036\n",
            "Iteration 145, loss = 0.70329510\n",
            "Iteration 146, loss = 0.70313025\n",
            "Iteration 147, loss = 0.70292907\n",
            "Iteration 148, loss = 0.70272552\n",
            "Iteration 149, loss = 0.70252536\n",
            "Iteration 150, loss = 0.70235667\n",
            "Iteration 151, loss = 0.70218308\n",
            "Iteration 152, loss = 0.70199941\n",
            "Iteration 153, loss = 0.70184124\n",
            "Iteration 154, loss = 0.70167468\n",
            "Iteration 155, loss = 0.70149895\n",
            "Iteration 156, loss = 0.70136311\n",
            "Iteration 157, loss = 0.70118693\n",
            "Iteration 158, loss = 0.70102240\n",
            "Iteration 159, loss = 0.70087557\n",
            "Iteration 160, loss = 0.70071840\n",
            "Iteration 161, loss = 0.70056790\n",
            "Iteration 162, loss = 0.70042559\n",
            "Iteration 163, loss = 0.70028091\n",
            "Iteration 164, loss = 0.70014508\n",
            "Iteration 165, loss = 0.70000900\n",
            "Iteration 166, loss = 0.69986247\n",
            "Iteration 167, loss = 0.69973874\n",
            "Iteration 168, loss = 0.69962544\n",
            "Iteration 169, loss = 0.69947797\n",
            "Iteration 170, loss = 0.69936132\n",
            "Iteration 171, loss = 0.69926987\n",
            "Iteration 172, loss = 0.69912788\n",
            "Iteration 173, loss = 0.69900456\n",
            "Iteration 174, loss = 0.69889498\n",
            "Iteration 175, loss = 0.69877833\n",
            "Iteration 176, loss = 0.69867149\n",
            "Iteration 177, loss = 0.69858593\n",
            "Iteration 178, loss = 0.69845959\n",
            "Iteration 179, loss = 0.69836698\n",
            "Iteration 180, loss = 0.69826655\n",
            "Iteration 181, loss = 0.69816029\n",
            "Iteration 182, loss = 0.69805974\n",
            "Iteration 183, loss = 0.69797349\n",
            "Iteration 184, loss = 0.69786856\n",
            "Iteration 185, loss = 0.69777942\n",
            "Iteration 186, loss = 0.69770740\n",
            "Iteration 187, loss = 0.69760813\n",
            "Iteration 188, loss = 0.69753268\n",
            "Iteration 189, loss = 0.69743602\n",
            "Iteration 190, loss = 0.69735318\n",
            "Iteration 191, loss = 0.69727579\n",
            "Iteration 192, loss = 0.69719209\n",
            "Iteration 193, loss = 0.69713149\n",
            "Iteration 194, loss = 0.69703710\n",
            "Iteration 195, loss = 0.69696428\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_DaeIMtFZ2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c0e2cc9-1f0f-44dd-c5fa-2d296e2581f1"
      },
      "source": [
        "results.append(['run1/relu TRAIN',clf.score(X_train, y_train)])\n",
        "clf.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5013333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vio9BYNsgEhC"
      },
      "source": [
        "# run2 & Used tanh as activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi0q1HMkE7QA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5be1e97c-de22-4e24-cf8e-f4a5d31ab7e6"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y = make_classification(n_samples=1000, random_state=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
        "clf = MLPClassifier(hidden_layer_sizes = (784,100,50,1),alpha=0.0001,solver='sgd', verbose=10,activation='tanh',max_iter=300,random_state=1).fit(X_train, y_train)\n",
        "clf.predict_proba(X_test)\n",
        "clf.predict(X_test)\n",
        "results.append(['run2/tanh TEST',clf.score(X_test, y_test)])\n",
        "clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.80920983\n",
            "Iteration 2, loss = 0.72994170\n",
            "Iteration 3, loss = 0.64361246\n",
            "Iteration 4, loss = 0.57180434\n",
            "Iteration 5, loss = 0.52345714\n",
            "Iteration 6, loss = 0.49035818\n",
            "Iteration 7, loss = 0.46749200\n",
            "Iteration 8, loss = 0.45221066\n",
            "Iteration 9, loss = 0.44161632\n",
            "Iteration 10, loss = 0.43484646\n",
            "Iteration 11, loss = 0.42965329\n",
            "Iteration 12, loss = 0.42577114\n",
            "Iteration 13, loss = 0.42262713\n",
            "Iteration 14, loss = 0.41971315\n",
            "Iteration 15, loss = 0.41754791\n",
            "Iteration 16, loss = 0.41528654\n",
            "Iteration 17, loss = 0.41329814\n",
            "Iteration 18, loss = 0.41164002\n",
            "Iteration 19, loss = 0.41011457\n",
            "Iteration 20, loss = 0.40858454\n",
            "Iteration 21, loss = 0.40739422\n",
            "Iteration 22, loss = 0.40614158\n",
            "Iteration 23, loss = 0.40487387\n",
            "Iteration 24, loss = 0.40382069\n",
            "Iteration 25, loss = 0.40265010\n",
            "Iteration 26, loss = 0.40165804\n",
            "Iteration 27, loss = 0.40076213\n",
            "Iteration 28, loss = 0.39979447\n",
            "Iteration 29, loss = 0.39877751\n",
            "Iteration 30, loss = 0.39800041\n",
            "Iteration 31, loss = 0.39712435\n",
            "Iteration 32, loss = 0.39636161\n",
            "Iteration 33, loss = 0.39553351\n",
            "Iteration 34, loss = 0.39483689\n",
            "Iteration 35, loss = 0.39399147\n",
            "Iteration 36, loss = 0.39330388\n",
            "Iteration 37, loss = 0.39256591\n",
            "Iteration 38, loss = 0.39192099\n",
            "Iteration 39, loss = 0.39121424\n",
            "Iteration 40, loss = 0.39058774\n",
            "Iteration 41, loss = 0.38997812\n",
            "Iteration 42, loss = 0.38929708\n",
            "Iteration 43, loss = 0.38877907\n",
            "Iteration 44, loss = 0.38815406\n",
            "Iteration 45, loss = 0.38752984\n",
            "Iteration 46, loss = 0.38707962\n",
            "Iteration 47, loss = 0.38642791\n",
            "Iteration 48, loss = 0.38587246\n",
            "Iteration 49, loss = 0.38534877\n",
            "Iteration 50, loss = 0.38481099\n",
            "Iteration 51, loss = 0.38433818\n",
            "Iteration 52, loss = 0.38384287\n",
            "Iteration 53, loss = 0.38336580\n",
            "Iteration 54, loss = 0.38288422\n",
            "Iteration 55, loss = 0.38245748\n",
            "Iteration 56, loss = 0.38190709\n",
            "Iteration 57, loss = 0.38145151\n",
            "Iteration 58, loss = 0.38100958\n",
            "Iteration 59, loss = 0.38061665\n",
            "Iteration 60, loss = 0.38012985\n",
            "Iteration 61, loss = 0.37971427\n",
            "Iteration 62, loss = 0.37930223\n",
            "Iteration 63, loss = 0.37885583\n",
            "Iteration 64, loss = 0.37850045\n",
            "Iteration 65, loss = 0.37805569\n",
            "Iteration 66, loss = 0.37763247\n",
            "Iteration 67, loss = 0.37727358\n",
            "Iteration 68, loss = 0.37693664\n",
            "Iteration 69, loss = 0.37648197\n",
            "Iteration 70, loss = 0.37614845\n",
            "Iteration 71, loss = 0.37579298\n",
            "Iteration 72, loss = 0.37541184\n",
            "Iteration 73, loss = 0.37503396\n",
            "Iteration 74, loss = 0.37467690\n",
            "Iteration 75, loss = 0.37434625\n",
            "Iteration 76, loss = 0.37399532\n",
            "Iteration 77, loss = 0.37367229\n",
            "Iteration 78, loss = 0.37330946\n",
            "Iteration 79, loss = 0.37292940\n",
            "Iteration 80, loss = 0.37264229\n",
            "Iteration 81, loss = 0.37235778\n",
            "Iteration 82, loss = 0.37198232\n",
            "Iteration 83, loss = 0.37164464\n",
            "Iteration 84, loss = 0.37138222\n",
            "Iteration 85, loss = 0.37101795\n",
            "Iteration 86, loss = 0.37078179\n",
            "Iteration 87, loss = 0.37050301\n",
            "Iteration 88, loss = 0.37010892\n",
            "Iteration 89, loss = 0.36981569\n",
            "Iteration 90, loss = 0.36951057\n",
            "Iteration 91, loss = 0.36924514\n",
            "Iteration 92, loss = 0.36889025\n",
            "Iteration 93, loss = 0.36862224\n",
            "Iteration 94, loss = 0.36835133\n",
            "Iteration 95, loss = 0.36811277\n",
            "Iteration 96, loss = 0.36773134\n",
            "Iteration 97, loss = 0.36748240\n",
            "Iteration 98, loss = 0.36723281\n",
            "Iteration 99, loss = 0.36697935\n",
            "Iteration 100, loss = 0.36663382\n",
            "Iteration 101, loss = 0.36642284\n",
            "Iteration 102, loss = 0.36612843\n",
            "Iteration 103, loss = 0.36584933\n",
            "Iteration 104, loss = 0.36553004\n",
            "Iteration 105, loss = 0.36532514\n",
            "Iteration 106, loss = 0.36503157\n",
            "Iteration 107, loss = 0.36473697\n",
            "Iteration 108, loss = 0.36450768\n",
            "Iteration 109, loss = 0.36422567\n",
            "Iteration 110, loss = 0.36395496\n",
            "Iteration 111, loss = 0.36377528\n",
            "Iteration 112, loss = 0.36358225\n",
            "Iteration 113, loss = 0.36319897\n",
            "Iteration 114, loss = 0.36296598\n",
            "Iteration 115, loss = 0.36273044\n",
            "Iteration 116, loss = 0.36244875\n",
            "Iteration 117, loss = 0.36223831\n",
            "Iteration 118, loss = 0.36197484\n",
            "Iteration 119, loss = 0.36167985\n",
            "Iteration 120, loss = 0.36155241\n",
            "Iteration 121, loss = 0.36118881\n",
            "Iteration 122, loss = 0.36097460\n",
            "Iteration 123, loss = 0.36077910\n",
            "Iteration 124, loss = 0.36044443\n",
            "Iteration 125, loss = 0.36026402\n",
            "Iteration 126, loss = 0.35995960\n",
            "Iteration 127, loss = 0.35976941\n",
            "Iteration 128, loss = 0.35955423\n",
            "Iteration 129, loss = 0.35930514\n",
            "Iteration 130, loss = 0.35900433\n",
            "Iteration 131, loss = 0.35883473\n",
            "Iteration 132, loss = 0.35852975\n",
            "Iteration 133, loss = 0.35834676\n",
            "Iteration 134, loss = 0.35820808\n",
            "Iteration 135, loss = 0.35788472\n",
            "Iteration 136, loss = 0.35768989\n",
            "Iteration 137, loss = 0.35738385\n",
            "Iteration 138, loss = 0.35712503\n",
            "Iteration 139, loss = 0.35691006\n",
            "Iteration 140, loss = 0.35669064\n",
            "Iteration 141, loss = 0.35642568\n",
            "Iteration 142, loss = 0.35626237\n",
            "Iteration 143, loss = 0.35594567\n",
            "Iteration 144, loss = 0.35574944\n",
            "Iteration 145, loss = 0.35551456\n",
            "Iteration 146, loss = 0.35527363\n",
            "Iteration 147, loss = 0.35508534\n",
            "Iteration 148, loss = 0.35478088\n",
            "Iteration 149, loss = 0.35474896\n",
            "Iteration 150, loss = 0.35441608\n",
            "Iteration 151, loss = 0.35410784\n",
            "Iteration 152, loss = 0.35385966\n",
            "Iteration 153, loss = 0.35366317\n",
            "Iteration 154, loss = 0.35342408\n",
            "Iteration 155, loss = 0.35317674\n",
            "Iteration 156, loss = 0.35291798\n",
            "Iteration 157, loss = 0.35278027\n",
            "Iteration 158, loss = 0.35251386\n",
            "Iteration 159, loss = 0.35222115\n",
            "Iteration 160, loss = 0.35209472\n",
            "Iteration 161, loss = 0.35176830\n",
            "Iteration 162, loss = 0.35159601\n",
            "Iteration 163, loss = 0.35136478\n",
            "Iteration 164, loss = 0.35113330\n",
            "Iteration 165, loss = 0.35085117\n",
            "Iteration 166, loss = 0.35063721\n",
            "Iteration 167, loss = 0.35039769\n",
            "Iteration 168, loss = 0.35014072\n",
            "Iteration 169, loss = 0.34999047\n",
            "Iteration 170, loss = 0.34969047\n",
            "Iteration 171, loss = 0.34953147\n",
            "Iteration 172, loss = 0.34919411\n",
            "Iteration 173, loss = 0.34901720\n",
            "Iteration 174, loss = 0.34880987\n",
            "Iteration 175, loss = 0.34855249\n",
            "Iteration 176, loss = 0.34827275\n",
            "Iteration 177, loss = 0.34801434\n",
            "Iteration 178, loss = 0.34781710\n",
            "Iteration 179, loss = 0.34758299\n",
            "Iteration 180, loss = 0.34727964\n",
            "Iteration 181, loss = 0.34709147\n",
            "Iteration 182, loss = 0.34684411\n",
            "Iteration 183, loss = 0.34660911\n",
            "Iteration 184, loss = 0.34649694\n",
            "Iteration 185, loss = 0.34615715\n",
            "Iteration 186, loss = 0.34621829\n",
            "Iteration 187, loss = 0.34572537\n",
            "Iteration 188, loss = 0.34537698\n",
            "Iteration 189, loss = 0.34520842\n",
            "Iteration 190, loss = 0.34498779\n",
            "Iteration 191, loss = 0.34468239\n",
            "Iteration 192, loss = 0.34450278\n",
            "Iteration 193, loss = 0.34419410\n",
            "Iteration 194, loss = 0.34395825\n",
            "Iteration 195, loss = 0.34368067\n",
            "Iteration 196, loss = 0.34347394\n",
            "Iteration 197, loss = 0.34329580\n",
            "Iteration 198, loss = 0.34297819\n",
            "Iteration 199, loss = 0.34285400\n",
            "Iteration 200, loss = 0.34255763\n",
            "Iteration 201, loss = 0.34231612\n",
            "Iteration 202, loss = 0.34202288\n",
            "Iteration 203, loss = 0.34188038\n",
            "Iteration 204, loss = 0.34159121\n",
            "Iteration 205, loss = 0.34136014\n",
            "Iteration 206, loss = 0.34109270\n",
            "Iteration 207, loss = 0.34088084\n",
            "Iteration 208, loss = 0.34053773\n",
            "Iteration 209, loss = 0.34039299\n",
            "Iteration 210, loss = 0.34013844\n",
            "Iteration 211, loss = 0.33987317\n",
            "Iteration 212, loss = 0.33953838\n",
            "Iteration 213, loss = 0.33925590\n",
            "Iteration 214, loss = 0.33909141\n",
            "Iteration 215, loss = 0.33881514\n",
            "Iteration 216, loss = 0.33854265\n",
            "Iteration 217, loss = 0.33840913\n",
            "Iteration 218, loss = 0.33807931\n",
            "Iteration 219, loss = 0.33782768\n",
            "Iteration 220, loss = 0.33758495\n",
            "Iteration 221, loss = 0.33733542\n",
            "Iteration 222, loss = 0.33704780\n",
            "Iteration 223, loss = 0.33680940\n",
            "Iteration 224, loss = 0.33649478\n",
            "Iteration 225, loss = 0.33631801\n",
            "Iteration 226, loss = 0.33606170\n",
            "Iteration 227, loss = 0.33580227\n",
            "Iteration 228, loss = 0.33551717\n",
            "Iteration 229, loss = 0.33528816\n",
            "Iteration 230, loss = 0.33543445\n",
            "Iteration 231, loss = 0.33476943\n",
            "Iteration 232, loss = 0.33466732\n",
            "Iteration 233, loss = 0.33434144\n",
            "Iteration 234, loss = 0.33398937\n",
            "Iteration 235, loss = 0.33382207\n",
            "Iteration 236, loss = 0.33347320\n",
            "Iteration 237, loss = 0.33323847\n",
            "Iteration 238, loss = 0.33296887\n",
            "Iteration 239, loss = 0.33274545\n",
            "Iteration 240, loss = 0.33243817\n",
            "Iteration 241, loss = 0.33219063\n",
            "Iteration 242, loss = 0.33201256\n",
            "Iteration 243, loss = 0.33162398\n",
            "Iteration 244, loss = 0.33142725\n",
            "Iteration 245, loss = 0.33111929\n",
            "Iteration 246, loss = 0.33089226\n",
            "Iteration 247, loss = 0.33059258\n",
            "Iteration 248, loss = 0.33044409\n",
            "Iteration 249, loss = 0.33006697\n",
            "Iteration 250, loss = 0.32990151\n",
            "Iteration 251, loss = 0.32967589\n",
            "Iteration 252, loss = 0.32935561\n",
            "Iteration 253, loss = 0.32917846\n",
            "Iteration 254, loss = 0.32877437\n",
            "Iteration 255, loss = 0.32855899\n",
            "Iteration 256, loss = 0.32824285\n",
            "Iteration 257, loss = 0.32801308\n",
            "Iteration 258, loss = 0.32778340\n",
            "Iteration 259, loss = 0.32742631\n",
            "Iteration 260, loss = 0.32722137\n",
            "Iteration 261, loss = 0.32698738\n",
            "Iteration 262, loss = 0.32676355\n",
            "Iteration 263, loss = 0.32642170\n",
            "Iteration 264, loss = 0.32619594\n",
            "Iteration 265, loss = 0.32587720\n",
            "Iteration 266, loss = 0.32565159\n",
            "Iteration 267, loss = 0.32534133\n",
            "Iteration 268, loss = 0.32507390\n",
            "Iteration 269, loss = 0.32484021\n",
            "Iteration 270, loss = 0.32456656\n",
            "Iteration 271, loss = 0.32430527\n",
            "Iteration 272, loss = 0.32406275\n",
            "Iteration 273, loss = 0.32383200\n",
            "Iteration 274, loss = 0.32346880\n",
            "Iteration 275, loss = 0.32325993\n",
            "Iteration 276, loss = 0.32301762\n",
            "Iteration 277, loss = 0.32271744\n",
            "Iteration 278, loss = 0.32251886\n",
            "Iteration 279, loss = 0.32229656\n",
            "Iteration 280, loss = 0.32193682\n",
            "Iteration 281, loss = 0.32171472\n",
            "Iteration 282, loss = 0.32138495\n",
            "Iteration 283, loss = 0.32116451\n",
            "Iteration 284, loss = 0.32087081\n",
            "Iteration 285, loss = 0.32060898\n",
            "Iteration 286, loss = 0.32031555\n",
            "Iteration 287, loss = 0.32016500\n",
            "Iteration 288, loss = 0.31982599\n",
            "Iteration 289, loss = 0.31948320\n",
            "Iteration 290, loss = 0.31920023\n",
            "Iteration 291, loss = 0.31907448\n",
            "Iteration 292, loss = 0.31870623\n",
            "Iteration 293, loss = 0.31850775\n",
            "Iteration 294, loss = 0.31821956\n",
            "Iteration 295, loss = 0.31789573\n",
            "Iteration 296, loss = 0.31766533\n",
            "Iteration 297, loss = 0.31739416\n",
            "Iteration 298, loss = 0.31705656\n",
            "Iteration 299, loss = 0.31680508\n",
            "Iteration 300, loss = 0.31663679\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS68zppMGCWA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a83a704-b221-43e6-fc35-399f8168855d"
      },
      "source": [
        "results.append(['run2/tanh TRAIN',clf.score(X_train, y_train)])\n",
        "clf.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.88"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INuGHxamgBTd"
      },
      "source": [
        "# run2 & Used relu as activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1uHgdDlfh4t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f62f3a5b-7de5-4583-cc29-259c387babc7"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y = make_classification(n_samples=1000, random_state=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
        "clf = MLPClassifier(hidden_layer_sizes = (784,100,50,1),alpha=0.0001,solver='sgd', verbose=10,activation='relu',max_iter=300,random_state=1).fit(X_train, y_train)\n",
        "clf.predict_proba(X_test)\n",
        "clf.predict(X_test)\n",
        "results.append(['run2/relu TEST',clf.score(X_test, y_test)])\n",
        "clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.85380795\n",
            "Iteration 2, loss = 0.82271797\n",
            "Iteration 3, loss = 0.79008357\n",
            "Iteration 4, loss = 0.77245506\n",
            "Iteration 5, loss = 0.76381726\n",
            "Iteration 6, loss = 0.76026356\n",
            "Iteration 7, loss = 0.75777491\n",
            "Iteration 8, loss = 0.75566388\n",
            "Iteration 9, loss = 0.75371077\n",
            "Iteration 10, loss = 0.75181926\n",
            "Iteration 11, loss = 0.74985394\n",
            "Iteration 12, loss = 0.74774754\n",
            "Iteration 13, loss = 0.74561089\n",
            "Iteration 14, loss = 0.74347815\n",
            "Iteration 15, loss = 0.74122719\n",
            "Iteration 16, loss = 0.73900917\n",
            "Iteration 17, loss = 0.73662253\n",
            "Iteration 18, loss = 0.73430268\n",
            "Iteration 19, loss = 0.73161098\n",
            "Iteration 20, loss = 0.72892396\n",
            "Iteration 21, loss = 0.72619814\n",
            "Iteration 22, loss = 0.72326499\n",
            "Iteration 23, loss = 0.72038726\n",
            "Iteration 24, loss = 0.71721387\n",
            "Iteration 25, loss = 0.71413666\n",
            "Iteration 26, loss = 0.71090003\n",
            "Iteration 27, loss = 0.70788845\n",
            "Iteration 28, loss = 0.70468495\n",
            "Iteration 29, loss = 0.70161702\n",
            "Iteration 30, loss = 0.69859720\n",
            "Iteration 31, loss = 0.69548463\n",
            "Iteration 32, loss = 0.69254441\n",
            "Iteration 33, loss = 0.68934722\n",
            "Iteration 34, loss = 0.68644852\n",
            "Iteration 35, loss = 0.68329643\n",
            "Iteration 36, loss = 0.68017852\n",
            "Iteration 37, loss = 0.67718664\n",
            "Iteration 38, loss = 0.67420200\n",
            "Iteration 39, loss = 0.67112320\n",
            "Iteration 40, loss = 0.66816031\n",
            "Iteration 41, loss = 0.66505869\n",
            "Iteration 42, loss = 0.66204048\n",
            "Iteration 43, loss = 0.65895793\n",
            "Iteration 44, loss = 0.65597183\n",
            "Iteration 45, loss = 0.65295891\n",
            "Iteration 46, loss = 0.65007857\n",
            "Iteration 47, loss = 0.64705361\n",
            "Iteration 48, loss = 0.64418641\n",
            "Iteration 49, loss = 0.64131305\n",
            "Iteration 50, loss = 0.63834066\n",
            "Iteration 51, loss = 0.63554304\n",
            "Iteration 52, loss = 0.63265705\n",
            "Iteration 53, loss = 0.62988668\n",
            "Iteration 54, loss = 0.62709224\n",
            "Iteration 55, loss = 0.62432492\n",
            "Iteration 56, loss = 0.62154481\n",
            "Iteration 57, loss = 0.61888930\n",
            "Iteration 58, loss = 0.61623715\n",
            "Iteration 59, loss = 0.61364761\n",
            "Iteration 60, loss = 0.61098101\n",
            "Iteration 61, loss = 0.60844283\n",
            "Iteration 62, loss = 0.60588509\n",
            "Iteration 63, loss = 0.60341229\n",
            "Iteration 64, loss = 0.60087383\n",
            "Iteration 65, loss = 0.59841420\n",
            "Iteration 66, loss = 0.59595143\n",
            "Iteration 67, loss = 0.59346232\n",
            "Iteration 68, loss = 0.59107961\n",
            "Iteration 69, loss = 0.58863237\n",
            "Iteration 70, loss = 0.58625797\n",
            "Iteration 71, loss = 0.58388418\n",
            "Iteration 72, loss = 0.58151196\n",
            "Iteration 73, loss = 0.57916654\n",
            "Iteration 74, loss = 0.57679997\n",
            "Iteration 75, loss = 0.57448806\n",
            "Iteration 76, loss = 0.57219665\n",
            "Iteration 77, loss = 0.56985085\n",
            "Iteration 78, loss = 0.56763753\n",
            "Iteration 79, loss = 0.56531554\n",
            "Iteration 80, loss = 0.56310587\n",
            "Iteration 81, loss = 0.56086955\n",
            "Iteration 82, loss = 0.55863491\n",
            "Iteration 83, loss = 0.55640847\n",
            "Iteration 84, loss = 0.55422970\n",
            "Iteration 85, loss = 0.55208527\n",
            "Iteration 86, loss = 0.54999867\n",
            "Iteration 87, loss = 0.54780127\n",
            "Iteration 88, loss = 0.54571381\n",
            "Iteration 89, loss = 0.54357137\n",
            "Iteration 90, loss = 0.54150987\n",
            "Iteration 91, loss = 0.53943272\n",
            "Iteration 92, loss = 0.53738698\n",
            "Iteration 93, loss = 0.53537189\n",
            "Iteration 94, loss = 0.53335283\n",
            "Iteration 95, loss = 0.53138375\n",
            "Iteration 96, loss = 0.52931731\n",
            "Iteration 97, loss = 0.52734450\n",
            "Iteration 98, loss = 0.52545076\n",
            "Iteration 99, loss = 0.52348909\n",
            "Iteration 100, loss = 0.52156723\n",
            "Iteration 101, loss = 0.51961203\n",
            "Iteration 102, loss = 0.51772024\n",
            "Iteration 103, loss = 0.51587890\n",
            "Iteration 104, loss = 0.51398436\n",
            "Iteration 105, loss = 0.51210869\n",
            "Iteration 106, loss = 0.51024625\n",
            "Iteration 107, loss = 0.50841861\n",
            "Iteration 108, loss = 0.50659549\n",
            "Iteration 109, loss = 0.50479529\n",
            "Iteration 110, loss = 0.50298339\n",
            "Iteration 111, loss = 0.50123690\n",
            "Iteration 112, loss = 0.49945998\n",
            "Iteration 113, loss = 0.49770830\n",
            "Iteration 114, loss = 0.49595308\n",
            "Iteration 115, loss = 0.49425581\n",
            "Iteration 116, loss = 0.49247613\n",
            "Iteration 117, loss = 0.49077560\n",
            "Iteration 118, loss = 0.48907484\n",
            "Iteration 119, loss = 0.48736153\n",
            "Iteration 120, loss = 0.48573356\n",
            "Iteration 121, loss = 0.48398255\n",
            "Iteration 122, loss = 0.48235055\n",
            "Iteration 123, loss = 0.48068316\n",
            "Iteration 124, loss = 0.47904386\n",
            "Iteration 125, loss = 0.47744909\n",
            "Iteration 126, loss = 0.47581388\n",
            "Iteration 127, loss = 0.47420426\n",
            "Iteration 128, loss = 0.47262416\n",
            "Iteration 129, loss = 0.47106879\n",
            "Iteration 130, loss = 0.46943199\n",
            "Iteration 131, loss = 0.46794566\n",
            "Iteration 132, loss = 0.46632087\n",
            "Iteration 133, loss = 0.46479271\n",
            "Iteration 134, loss = 0.46333341\n",
            "Iteration 135, loss = 0.46181320\n",
            "Iteration 136, loss = 0.46024657\n",
            "Iteration 137, loss = 0.45875356\n",
            "Iteration 138, loss = 0.45729395\n",
            "Iteration 139, loss = 0.45579825\n",
            "Iteration 140, loss = 0.45430311\n",
            "Iteration 141, loss = 0.45282221\n",
            "Iteration 142, loss = 0.45146301\n",
            "Iteration 143, loss = 0.44997602\n",
            "Iteration 144, loss = 0.44851175\n",
            "Iteration 145, loss = 0.44710555\n",
            "Iteration 146, loss = 0.44568183\n",
            "Iteration 147, loss = 0.44428546\n",
            "Iteration 148, loss = 0.44287716\n",
            "Iteration 149, loss = 0.44154931\n",
            "Iteration 150, loss = 0.44016643\n",
            "Iteration 151, loss = 0.43872160\n",
            "Iteration 152, loss = 0.43737491\n",
            "Iteration 153, loss = 0.43604471\n",
            "Iteration 154, loss = 0.43470396\n",
            "Iteration 155, loss = 0.43333959\n",
            "Iteration 156, loss = 0.43201766\n",
            "Iteration 157, loss = 0.43069972\n",
            "Iteration 158, loss = 0.42938742\n",
            "Iteration 159, loss = 0.42807700\n",
            "Iteration 160, loss = 0.42677283\n",
            "Iteration 161, loss = 0.42547468\n",
            "Iteration 162, loss = 0.42421151\n",
            "Iteration 163, loss = 0.42297322\n",
            "Iteration 164, loss = 0.42165310\n",
            "Iteration 165, loss = 0.42040997\n",
            "Iteration 166, loss = 0.41914501\n",
            "Iteration 167, loss = 0.41790256\n",
            "Iteration 168, loss = 0.41669779\n",
            "Iteration 169, loss = 0.41547519\n",
            "Iteration 170, loss = 0.41420876\n",
            "Iteration 171, loss = 0.41303852\n",
            "Iteration 172, loss = 0.41178987\n",
            "Iteration 173, loss = 0.41065705\n",
            "Iteration 174, loss = 0.40939883\n",
            "Iteration 175, loss = 0.40823498\n",
            "Iteration 176, loss = 0.40703937\n",
            "Iteration 177, loss = 0.40587353\n",
            "Iteration 178, loss = 0.40472627\n",
            "Iteration 179, loss = 0.40349635\n",
            "Iteration 180, loss = 0.40236503\n",
            "Iteration 181, loss = 0.40127116\n",
            "Iteration 182, loss = 0.40004333\n",
            "Iteration 183, loss = 0.39891477\n",
            "Iteration 184, loss = 0.39779112\n",
            "Iteration 185, loss = 0.39666234\n",
            "Iteration 186, loss = 0.39566215\n",
            "Iteration 187, loss = 0.39437905\n",
            "Iteration 188, loss = 0.39328607\n",
            "Iteration 189, loss = 0.39218771\n",
            "Iteration 190, loss = 0.39107924\n",
            "Iteration 191, loss = 0.39003187\n",
            "Iteration 192, loss = 0.38893197\n",
            "Iteration 193, loss = 0.38782262\n",
            "Iteration 194, loss = 0.38679172\n",
            "Iteration 195, loss = 0.38566271\n",
            "Iteration 196, loss = 0.38458797\n",
            "Iteration 197, loss = 0.38358782\n",
            "Iteration 198, loss = 0.38246572\n",
            "Iteration 199, loss = 0.38147323\n",
            "Iteration 200, loss = 0.38042392\n",
            "Iteration 201, loss = 0.37937664\n",
            "Iteration 202, loss = 0.37833498\n",
            "Iteration 203, loss = 0.37735668\n",
            "Iteration 204, loss = 0.37629331\n",
            "Iteration 205, loss = 0.37530374\n",
            "Iteration 206, loss = 0.37429559\n",
            "Iteration 207, loss = 0.37333873\n",
            "Iteration 208, loss = 0.37229492\n",
            "Iteration 209, loss = 0.37123347\n",
            "Iteration 210, loss = 0.37030290\n",
            "Iteration 211, loss = 0.36926629\n",
            "Iteration 212, loss = 0.36826332\n",
            "Iteration 213, loss = 0.36724571\n",
            "Iteration 214, loss = 0.36630304\n",
            "Iteration 215, loss = 0.36533459\n",
            "Iteration 216, loss = 0.36441290\n",
            "Iteration 217, loss = 0.36339086\n",
            "Iteration 218, loss = 0.36239481\n",
            "Iteration 219, loss = 0.36150056\n",
            "Iteration 220, loss = 0.36050553\n",
            "Iteration 221, loss = 0.35955165\n",
            "Iteration 222, loss = 0.35865785\n",
            "Iteration 223, loss = 0.35766533\n",
            "Iteration 224, loss = 0.35668253\n",
            "Iteration 225, loss = 0.35580841\n",
            "Iteration 226, loss = 0.35484617\n",
            "Iteration 227, loss = 0.35398887\n",
            "Iteration 228, loss = 0.35304851\n",
            "Iteration 229, loss = 0.35208102\n",
            "Iteration 230, loss = 0.35127230\n",
            "Iteration 231, loss = 0.35028465\n",
            "Iteration 232, loss = 0.34937171\n",
            "Iteration 233, loss = 0.34850246\n",
            "Iteration 234, loss = 0.34753139\n",
            "Iteration 235, loss = 0.34667332\n",
            "Iteration 236, loss = 0.34570860\n",
            "Iteration 237, loss = 0.34489510\n",
            "Iteration 238, loss = 0.34394986\n",
            "Iteration 239, loss = 0.34317219\n",
            "Iteration 240, loss = 0.34221561\n",
            "Iteration 241, loss = 0.34131002\n",
            "Iteration 242, loss = 0.34041088\n",
            "Iteration 243, loss = 0.33950361\n",
            "Iteration 244, loss = 0.33863146\n",
            "Iteration 245, loss = 0.33775259\n",
            "Iteration 246, loss = 0.33686858\n",
            "Iteration 247, loss = 0.33597974\n",
            "Iteration 248, loss = 0.33510428\n",
            "Iteration 249, loss = 0.33424210\n",
            "Iteration 250, loss = 0.33337609\n",
            "Iteration 251, loss = 0.33261658\n",
            "Iteration 252, loss = 0.33162302\n",
            "Iteration 253, loss = 0.33080164\n",
            "Iteration 254, loss = 0.32997497\n",
            "Iteration 255, loss = 0.32906384\n",
            "Iteration 256, loss = 0.32822860\n",
            "Iteration 257, loss = 0.32734859\n",
            "Iteration 258, loss = 0.32650250\n",
            "Iteration 259, loss = 0.32569237\n",
            "Iteration 260, loss = 0.32484196\n",
            "Iteration 261, loss = 0.32403456\n",
            "Iteration 262, loss = 0.32316832\n",
            "Iteration 263, loss = 0.32234779\n",
            "Iteration 264, loss = 0.32151828\n",
            "Iteration 265, loss = 0.32069148\n",
            "Iteration 266, loss = 0.31989795\n",
            "Iteration 267, loss = 0.31902884\n",
            "Iteration 268, loss = 0.31822567\n",
            "Iteration 269, loss = 0.31742401\n",
            "Iteration 270, loss = 0.31659275\n",
            "Iteration 271, loss = 0.31579784\n",
            "Iteration 272, loss = 0.31501356\n",
            "Iteration 273, loss = 0.31416976\n",
            "Iteration 274, loss = 0.31335966\n",
            "Iteration 275, loss = 0.31260253\n",
            "Iteration 276, loss = 0.31181109\n",
            "Iteration 277, loss = 0.31099344\n",
            "Iteration 278, loss = 0.31024120\n",
            "Iteration 279, loss = 0.30943203\n",
            "Iteration 280, loss = 0.30861117\n",
            "Iteration 281, loss = 0.30785271\n",
            "Iteration 282, loss = 0.30702296\n",
            "Iteration 283, loss = 0.30632782\n",
            "Iteration 284, loss = 0.30549846\n",
            "Iteration 285, loss = 0.30473695\n",
            "Iteration 286, loss = 0.30394240\n",
            "Iteration 287, loss = 0.30319018\n",
            "Iteration 288, loss = 0.30241376\n",
            "Iteration 289, loss = 0.30158263\n",
            "Iteration 290, loss = 0.30086588\n",
            "Iteration 291, loss = 0.30009042\n",
            "Iteration 292, loss = 0.29928668\n",
            "Iteration 293, loss = 0.29856225\n",
            "Iteration 294, loss = 0.29784602\n",
            "Iteration 295, loss = 0.29705035\n",
            "Iteration 296, loss = 0.29623932\n",
            "Iteration 297, loss = 0.29547057\n",
            "Iteration 298, loss = 0.29473057\n",
            "Iteration 299, loss = 0.29398570\n",
            "Iteration 300, loss = 0.29326427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.836"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hkmGKRUfi2m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1f23c10-9011-4268-a187-8ab11dc9b0ac"
      },
      "source": [
        "results.append(['run2/relu TRAIN',clf.score(X_train, y_train)])\n",
        "clf.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9373333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkSlX6i0rKQC"
      },
      "source": [
        "FINAL RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFDt17ONrJ-9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4d6cd42e-5d00-4f2f-ba8c-1ff0b8cad2e6"
      },
      "source": [
        "for output in results:\n",
        "  print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['run1/tanh TEST', 0.808]\n",
            "['run1/tanh TRAIN', 0.8773333333333333]\n",
            "['run1/relu TEST', 0.5]\n",
            "['run1/relu TRAIN', 0.5013333333333333]\n",
            "['run2/tanh TEST', 0.832]\n",
            "['run2/tanh TRAIN', 0.88]\n",
            "['run2/relu TEST', 0.836]\n",
            "['run2/relu TRAIN', 0.9373333333333334]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTWl_Vuit-5_"
      },
      "source": [
        "#Conclusion\n",
        "###RUN 1\n",
        "I used 784 as size of input layer in each trial since our picture size is 28x28\n",
        "Then i used 300 and 100 neurons for hidden layer sizes.<br>\n",
        "When i worked with tanh as activation function, It worked pretty well. Ofcourse Test set had a little worse score.<br>\n",
        "But when we look at the results of relu function, it seems like there is no improvement. The reason is probably it became a dead ReLU. So gradient descent learning will not alter the weights. \"Leaky\" ReLU with a small positive gradient for negative inputs may be the solution in this case. So the worst result i get was from run1 & relu\n",
        "<br><br>\n",
        "\n",
        "###RUN 2\n",
        "I've used 100 and 50 neurons in this case. When i examine the results for both relu and tanh, it seems like using this neuron amounts was better.<br>\n",
        "Probably some part of neurons were unnecessary in the previous case since its improved when i used less.<br>\n",
        "When i used relu and 100 to 50 neurons in hidden layers, i've got the best result."
      ]
    }
  ]
}