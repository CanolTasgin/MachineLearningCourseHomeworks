# -*- coding: utf-8 -*-
"""CS412_HW5_ccanol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BOqBXvhD0zOnpWPrJF8DdXad4XUV-W9W

# CS412 Homework 5
Celal Canol Taşgın / 20761 / ccanol@sabanciuniv.edu
"""

from keras.datasets import mnist
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# Flatten the images

image_vector_size = 28*28

x_train = x_train.reshape(x_train.shape[0], image_vector_size)

x_test = x_test.reshape(x_test.shape[0], image_vector_size)

"""# run1 & Used tanh as activation function"""

results = []
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
X, y = make_classification(n_samples=1000, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)
clf = MLPClassifier(hidden_layer_sizes = (784,300,100,1),alpha=0.0001,solver='sgd', verbose=10,activation='tanh',max_iter=300,random_state=1).fit(X_train, y_train)
clf.predict_proba(X_test)
clf.predict(X_test)
results.append(['run1/tanh TEST',clf.score(X_test, y_test)])
clf.score(X_test, y_test)

results.append(['run1/tanh TRAIN',clf.score(X_train, y_train)])
clf.score(X_train, y_train)

"""# run1 & Used relu as activation function"""

from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
X, y = make_classification(n_samples=1000, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)
clf = MLPClassifier(hidden_layer_sizes = (784,300,100,1),alpha=0.0001,solver='sgd', verbose=10,activation='relu',max_iter=300,random_state=1).fit(X_train, y_train)
clf.predict_proba(X_test)
clf.predict(X_test)
results.append(['run1/relu TEST',clf.score(X_test, y_test)])
clf.score(X_test, y_test)

results.append(['run1/relu TRAIN',clf.score(X_train, y_train)])
clf.score(X_train, y_train)

"""# run2 & Used tanh as activation function"""

from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
X, y = make_classification(n_samples=1000, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)
clf = MLPClassifier(hidden_layer_sizes = (784,100,50,1),alpha=0.0001,solver='sgd', verbose=10,activation='tanh',max_iter=300,random_state=1).fit(X_train, y_train)
clf.predict_proba(X_test)
clf.predict(X_test)
results.append(['run2/tanh TEST',clf.score(X_test, y_test)])
clf.score(X_test, y_test)

results.append(['run2/tanh TRAIN',clf.score(X_train, y_train)])
clf.score(X_train, y_train)

"""# run2 & Used relu as activation function"""

from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
X, y = make_classification(n_samples=1000, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)
clf = MLPClassifier(hidden_layer_sizes = (784,100,50,1),alpha=0.0001,solver='sgd', verbose=10,activation='relu',max_iter=300,random_state=1).fit(X_train, y_train)
clf.predict_proba(X_test)
clf.predict(X_test)
results.append(['run2/relu TEST',clf.score(X_test, y_test)])
clf.score(X_test, y_test)

results.append(['run2/relu TRAIN',clf.score(X_train, y_train)])
clf.score(X_train, y_train)

"""FINAL RESULTS"""

for output in results:
  print(output)

"""#Conclusion
###RUN 1
I used 784 as size of input layer in each trial since our picture size is 28x28
Then i used 300 and 100 neurons for hidden layer sizes.<br>
When i worked with tanh as activation function, It worked pretty well. Ofcourse Test set had a little worse score.<br>
But when we look at the results of relu function, it seems like there is no improvement. The reason is probably it became a dead ReLU. So gradient descent learning will not alter the weights. "Leaky" ReLU with a small positive gradient for negative inputs may be the solution in this case. So the worst result i get was from run1 & relu
<br><br>

###RUN 2
I've used 100 and 50 neurons in this case. When i examine the results for both relu and tanh, it seems like using this neuron amounts was better.<br>
Probably some part of neurons were unnecessary in the previous case since its improved when i used less.<br>
When i used relu and 100 to 50 neurons in hidden layers, i've got the best result.
"""