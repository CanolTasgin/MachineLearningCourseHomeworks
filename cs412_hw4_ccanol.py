# -*- coding: utf-8 -*-
"""CS412_HW4_ccanol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v3rwpw3hJo524729anb90Zsj6hmvZggx

# CS412 Homework 4
Celal Canol Taşgın / 20761 / ccanol@sabanciuniv.edu

# kMeans Algorithm
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

"""Read iris data set with pandas:"""

url="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
df = pd.read_csv(url, header = None, names =['Sepal length (cm)', 
                                             'Sepal width (cm)', 
                                             'Petal length (cm)',  
                                             'Petal width (cm)', 
                                             'Class'])
df.head(10)
df.info

"""Divided dataset and then """

x = df.iloc[:, [0,1,2,3]].values

"""Tried KMeans algorithm with clusters 1 to 10. Then used elbow method to see the best k for number of clusters. It seems like when k = 3, its best for this dataset."""

Error =[]
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i).fit(x)
    kmeans.fit(x)
    Error.append(kmeans.inertia_)
import matplotlib.pyplot as plt
plt.plot(range(1, 11), Error)
plt.title('Elbow method')
plt.xlabel('No of clusters')
plt.ylabel('Error')
plt.show()

"""# Tried n_clusters = 1 and n_init = 10"""

kmeans = KMeans(n_clusters=1,  n_init=10, random_state=0).fit(x)
y_kmeans = kmeans.fit_predict(x)
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'black', label = 'Centroids')

plt.legend()

abc = ['LABELS:',kmeans.labels_ ,'Cluster Centers:', kmeans.cluster_centers_ ,'INERTIA', kmeans.inertia_ ]
abc

"""# Tried n_clusters = 3 and n_init = 10"""

kmeans = KMeans(n_clusters=3,  n_init=10, random_state=0).fit(x)
y_kmeans = kmeans.fit_predict(x)
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'black', label = 'Centroids')

plt.legend()

abc = ['LABELS:',kmeans.labels_ ,'Cluster Centers:', kmeans.cluster_centers_ ,'INERTIA', kmeans.inertia_ ]
abc

"""# Tried n_clusters = 10 and n_init = 10"""

kmeans = KMeans(n_clusters=10,  n_init=10, random_state=0).fit(x)
y_kmeans = kmeans.fit_predict(x)
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'black', label = 'Centroids')

plt.legend()

abc = ['LABELS:',kmeans.labels_ ,'Cluster Centers:', kmeans.cluster_centers_ ,'INERTIA', kmeans.inertia_ ]
abc

"""# Tried n_clusters = 50 and n_init = 10"""

kmeans = KMeans(n_clusters=50,  n_init=10, random_state=0).fit(x)
y_kmeans = kmeans.fit_predict(x)
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'black', label = 'Centroids')

plt.legend()

abc = ['LABELS:',kmeans.labels_ ,'Cluster Centers:', kmeans.cluster_centers_ ,'INERTIA', kmeans.inertia_ ]
abc

"""# Tried n_clusters = 149 and n_init = 10"""

kmeans = KMeans(n_clusters=149,  n_init=10, random_state=0).fit(x)
y_kmeans = kmeans.fit_predict(x)
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'black', label = 'Centroids')

plt.legend()

abc = ['LABELS:',kmeans.labels_ ,'Cluster Centers:', kmeans.cluster_centers_ ,'INERTIA', kmeans.inertia_ ]
abc

"""# Tried n_clusters = 1 and n_init = 1 (Since it seems like n_cluster=3 is the best option)"""

kmeans = KMeans(n_clusters=3,  n_init=1, random_state=0).fit(x)
y_kmeans = kmeans.fit_predict(x)
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'black', label = 'Centroids')

plt.legend()

abc = ['LABELS:',kmeans.labels_ ,'Cluster Centers:', kmeans.cluster_centers_ ,'INERTIA', kmeans.inertia_ ]
abc

"""#CONCLUSION
Since the K-means algorithm aims to choose centroids that minimise the inertia, i've tried different cluster numbers and examined the inertia results, labels and cluster centers.
Here is my observation:
##1 Cluster
Since my aim is to split data into different clusters and predict the groups in the data, using 1 cluster was meaningless. Inertia was too high and labels don't help us to understand the data
##3 Clusters
I've checked 3 Cluster version before with the elbow method and saw its the best option. And obviously we know that its the best because this dataset have 3 classes. But kMeans is an unsupervised algorithm, so when we evaluate the inertia results, its much more better than 1 cluster version. Also when we examine the labels they seems fit to this dataset since originally there are 3 classes. But the error rate is not so low. Also when we used n_init=1 centroids where different and worse than the version with n_init=10
##10 Clusters
I see that its better for the inertia results when we increase the number of clusters. But since there are so many resulting label types, using the data become harder each time
##50 Clusters
Inertia is much more lower and predictions are better but there are meaninglessly many classes so this dataset is useless in the sense of machine learning
##149 Clusters
Every data have different label and there is no prediction anymore. Each data sample has its own cluster so there is no inertia here. Also since all data have different labels, this version of the algorithm is completely useless because its not changing anything in understandibility of the dataset
"""